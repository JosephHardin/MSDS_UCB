{"cells":[{"cell_type":"markdown","metadata":{"id":"ZcU5XMxHW35O"},"source":["# Network Analysis Final"]},{"cell_type":"markdown","metadata":{"id":"nsiaZU2h8MIs"},"source":["## ‚ö°Ô∏è Semantic Network Graph\n","\n","Next, you'll create a semantic network analysis graph of words used in Tweets. Practically, this graph will reveal what words are most commonly associated with each other, for each brand. You will create one semantic graph, and that graph will have the data for all three brands.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DrCzRxkb8G02"},"source":[]},{"cell_type":"markdown","metadata":{"id":"H0FWKf20XNAT"},"source":["In this lab, you will build a semantic network of Tweets. That is, a graph of Tweets related by natural language features of the Tweet texts.\n"]},{"cell_type":"markdown","metadata":{"id":"fGck_iPZ83ZT"},"source":["## ‚ö°Ô∏è Sentiment Segmentation\n","\n","Create additional networks of Twitter data by segmenting out subsets that unveil additional insights. Run Tweets through a natural language tool and extract negative and positive tweets. Do networks of these subsets.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3gNOVxdOLfr1"},"source":["#Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3831,"status":"ok","timestamp":1678211388903,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"VEr-dIfv90fP"},"outputs":[],"source":["import gzip\n","import re\n","import itertools\n","import json\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","import nltk\n","import string"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":961,"status":"ok","timestamp":1678211389862,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"Q5R-OM2e5xVF","outputId":"0b853237-c9af-4d2b-fa15-576810687e1e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download(\"punkt\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1678211389863,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"W6QKZL4OYkft","outputId":"326a03f2-0567-47d5-9c5c-4791aa024385"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download(\"stopwords\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1678211389863,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"tZPlW-2HAbHB","outputId":"57f0be0e-1aef-41d4-fa01-7d4b3ef7473a"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1678211389864,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"PvcS9J9Eke4K","outputId":"53a61955-e935-4cd8-c2ed-06c9e107ef04"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"data":{"text/plain":["True"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download(\"wordnet\")"]},{"cell_type":"markdown","metadata":{"id":"2qLFOpRZZjuh"},"source":["## Get the data"]},{"cell_type":"markdown","metadata":{"id":"w1oAfXlkZeOX"},"source":["Be sure you still have the brand Tweets file on your Google Drive from the previous Lab."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1678211389864,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"gdigBPlKTDeS"},"outputs":[],"source":["DATA_FILE = \"drive/MyDrive/nikelululemonadidas_tweets.jsonl.gz\""]},{"cell_type":"markdown","metadata":{"id":"zLsas-r3p6ws"},"source":["## Mount Google Drive"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16376,"status":"ok","timestamp":1678211406233,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"EjOJUUbdp9EJ","outputId":"4340b0f8-409c-4065-ab14-dce79a943af1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"OgBGBHTBLl-j"},"source":["## Text processing functions"]},{"cell_type":"markdown","metadata":{"id":"3Y57sZOd4Qqs"},"source":["#### A super-simple tokenizer"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1678211406235,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"pZBv5GUbiF_W"},"outputs":[],"source":["TWEET_TOKENIZER = nltk.TweetTokenizer().tokenize\n","WORD_TOKENIZER = nltk.tokenize.word_tokenize\n","\n","def tokenize(text, lowercase=True, tweet=False):\n","    \"\"\"Tokenize the text. By default, also normalizes text to lowercase.\n","    Optionally uses the Tweet Tokenizer.\n","    \"\"\"\n","    if lowercase:\n","        text = text.lower()\n","    if tweet:\n","        return TWEET_TOKENIZER(text)\n","    else:\n","        return WORD_TOKENIZER(text)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1678211406236,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"gs_pBc5IJTMY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30568,"status":"ok","timestamp":1678211436793,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"dGZpQt_A90fk","outputId":"54cb9129-8e6f-4e65-e113-b1e479315529"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 tweets processed\n","10000 tweets processed\n","20000 tweets processed\n","30000 tweets processed\n","40000 tweets processed\n","50000 tweets processed\n","60000 tweets processed\n","70000 tweets processed\n","80000 tweets processed\n","90000 tweets processed\n","100000 tweets processed\n","110000 tweets processed\n","120000 tweets processed\n","130000 tweets processed\n","140000 tweets processed\n","150000 tweets processed\n","160000 tweets processed\n","170000 tweets processed\n","175077 total Tweets processed\n"]}],"source":["users = {}\n","\n","with gzip.open(DATA_FILE) as data_file:\n","    for i, line in enumerate(data_file):\n","        if i % 10000 == 0: # Show a periodic status\n","            print(\"%s tweets processed\" % i)\n","        tweet = json.loads(line)\n","        user = tweet[\"user\"]\n","        user_id = user[\"id\"]\n","        if user_id not in users:\n","            users[user_id] = {\n","                \"id\": user_id,\n","                \"tweet_count\": 0,\n","                \"followers_count\": user[\"followers_count\"]\n","            }\n","        users[user_id][\"tweet_count\"] += 1\n","    print(f\"{i} total Tweets processed\")"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":36,"status":"ok","timestamp":1678211436793,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"THyEo8iVBvlG"},"outputs":[],"source":["included_user_ids = []\n","\n","min_tweet_count = 2\n","min_followers_count = 100000\n","\n","for user_id, user in users.items():\n","    if user[\"tweet_count\"] \u003e= min_tweet_count and \\\n","             user[\"followers_count\"] \u003e= min_followers_count:\n","        included_user_ids.append(user_id)"]},{"cell_type":"markdown","metadata":{"id":"l9OaEUqP43Bz"},"source":["## ‚ö°Ô∏è Twitter Mentions Graph\n","\n","Using python you must create a valued, directed network graph of twitter mentions. Your graph will show Twitter users that are most centrally related to the brand (e.g., they regularly mention the brand). This graph will also illustrate who mentions who on Twitter, and in what way those mentions flow. You will create one mention graph, and that mention graph will have mentions for all three brands.\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1038,"status":"ok","timestamp":1678211437798,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"uSM9x3jsS5ar"},"outputs":[],"source":["graph = nx.DiGraph()"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29642,"status":"ok","timestamp":1678211467432,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"ogdYP7BKQr0W","outputId":"2b0ca0e2-6f3a-465d-d08c-d27886086e24"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 tweets processed\n","10000 tweets processed\n","20000 tweets processed\n","30000 tweets processed\n","40000 tweets processed\n","50000 tweets processed\n","60000 tweets processed\n","70000 tweets processed\n","80000 tweets processed\n","90000 tweets processed\n","100000 tweets processed\n","110000 tweets processed\n","120000 tweets processed\n","130000 tweets processed\n","140000 tweets processed\n","150000 tweets processed\n","160000 tweets processed\n","170000 tweets processed\n"]}],"source":["with gzip.open(DATA_FILE) as data_file:\n","    for i, line in enumerate(data_file):\n","        if i % 10000 == 0:\n","            print(\"%s tweets processed\" % i)\n","        tweet = json.loads(line)\n","        sender_id = tweet[\"user\"][\"id\"]\n","        sender_name = tweet[\"user\"][\"screen_name\"]\n","        if sender_id in included_user_ids:\n","            for mention in tweet[\"entities\"][\"user_mentions\"]:\n","                receiver_name = mention[\"screen_name\"]\n","                receiver_id = mention[\"id\"]\n","                if receiver_id in included_user_ids:\n","                    graph.add_edge(sender_name, receiver_name)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1678211467433,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"xUPHmUmuH_F_"},"outputs":[],"source":["#nx.info(graph)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"16gI7_hVvTHvHuYi6drojcYo5TJsoQKw_"},"executionInfo":{"elapsed":44719,"status":"ok","timestamp":1678211512148,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"zbhvxJFle9TU","outputId":"05bee8c8-846b-4b08-87f1-e4aac08f9607"},"outputs":[],"source":["fig, ax = plt.subplots(1, 1, figsize=(300, 300))\n","nx.draw_networkx(graph, ax=ax, font_color=\"#FFFFFF\", font_size=20, node_size=30000, width=4, arrowsize=100)"]},{"cell_type":"markdown","metadata":{"id":"xsE_gYZ_OPi5"},"source":["### Lemmatizing with POS\n","\n","The following code snippets demonstrate differences in signaling the part-of-speech to the lemmatizer. The WordNet lemmatizer defaults to treating everything as nouns, which we will simply accept as good enough for the purpose of this lab."]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1678211512149,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"DWeAvcPuimE6"},"outputs":[],"source":["STEMMER = nltk.PorterStemmer()\n","\n","def stem(tokens):\n","    \"\"\"Stem the tokens. I.e., remove morphological affixes and\n","    normalize to standardized stem forms.\n","\n","    Has the side effective of producing \"unnatural\" forms due to\n","    stemming standards. E.g. quickly becomes quickli\n","    \"\"\"\n","    return [ STEMMER.stem(token) for token in tokens ]"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1678211512149,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"t0BqdhzMjzs1"},"outputs":[],"source":["LEMMATIZER = nltk.WordNetLemmatizer()\n","\n","def lemmatize(tokens):\n","    \"\"\"Lemmatize the tokens.\n","    \n","    Retains more natural word forms than stemming, but assumes all\n","    tokens are nouns unless tokens are passed as (word, pos) tuples.\n","    \"\"\"\n","    lemmas = []\n","    for token in tokens:\n","        if isinstance(token, str):\n","            lemmas.append(LEMMATIZER.lemmatize(token)) # treats token like a noun\n","        else: # assume a tuple of (word, pos)\n","            lemmas.append(LEMMATIZER.lemmatize(*token))\n","    return lemmas"]},{"cell_type":"markdown","metadata":{"id":"0ZA7YtMFSdS7"},"source":["### Removing stopwords"]},{"cell_type":"markdown","metadata":{"id":"t2q1-yPuSiId"},"source":["It can be useful to remove so-called stopwords to improve the average salience of the terms we are analyzing.\n","\n","Stop words tend to be things like articles and conjunctions that usually don't offer a lot of value in an analysis.\n","\n","The NLTK has a corpus of stopwords, but we'll include the option of passing in a custom list if desired."]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1678211512150,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"VDCC9UN7jBkb"},"outputs":[],"source":["def remove_stopwords(tokens, stopwords=None):\n","    \"\"\"Remove stopwords, i.e. words that we don't want as part of our\n","    analysis. Defaults to the default set of nltk english stopwords.\n","    \"\"\"\n","    if stopwords is None:\n","        stopwords = nltk.corpus.stopwords.words(\"english\")\n","    return [ token for token in tokens if token not in stopwords ]"]},{"cell_type":"markdown","metadata":{"id":"tPV4VnECgSYb"},"source":["### Removing hyperlinks"]},{"cell_type":"markdown","metadata":{"id":"nHfyiGC_gXEH"},"source":["Unless your analysis involves looking at what users are linking to (a more difficult and involved task than it might seem), then you might want to simply get those links out of the way."]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1678211512151,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"1TRqS0G6gwdd"},"outputs":[],"source":["def remove_links(tokens):\n","    \"\"\"Removes http/s links from the tokens.\n","\n","    This simple implementation assumes links have been kept intact as whole\n","    tokens. E.g. the way the Tweet Tokenizer works.\n","    \"\"\"\n","    return [ t for t in tokens\n","            if not t.startswith(\"http://\")\n","            and not t.startswith(\"https://\")\n","        ]\n"]},{"cell_type":"markdown","metadata":{"id":"odmLOVBsTjLc"},"source":["### Removing punctuation"]},{"cell_type":"markdown","metadata":{"id":"y72_TvcXbUCu"},"source":["Finally, for our purposes of analysis, we are really only interested in words, not punctuation. Here, we simply remove tokens that are punctuation."]},{"cell_type":"markdown","metadata":{"id":"Rn-aNiwLTnB5"},"source":["Tweets can get pretty messy, so we've gone beyond simply removing punctation tokens and decided to clean out punctuation altogether."]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1678211512151,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"NYj-wFTxlN-P"},"outputs":[],"source":["def remove_punctuation(tokens,\n","                       strip_mentions=False,\n","                       strip_hashtags=False,\n","                       strict=False):\n","    \"\"\"Remove punctuation from a list of tokens.\n","\n","    Has some specialized options for dealing with Tweets:\n","\n","    strip_mentions=True will strip the @ off of @ mentions\n","    strip_hashtags=True will strip the # from hashtags\n","\n","    strict=True will remove all punctuation from all tokens, not merely\n","    just tokens that are punctuation per se. \n","    \"\"\"\n","    tokens = [t for t in tokens if t not in string.punctuation]\n","    if strip_mentions:\n","        tokens = [t.lstrip('@') for t in tokens]\n","    if strip_hashtags:\n","        tokens = [t.lstrip('#') for t in tokens]\n","    if strict:\n","        cleaned = []\n","        for t in tokens:\n","            cleaned.append(\n","                t.translate(str.maketrans('', '', string.punctuation)).strip())\n","        tokens = [t for t in cleaned if t]\n","    return tokens"]},{"cell_type":"markdown","metadata":{"id":"wUz7hLKSf2eu"},"source":["## Finally working with the data"]},{"cell_type":"markdown","metadata":{"id":"zMgCzDn1f5pJ"},"source":["Data cleanup is a big task and ultimately one of the bigger burdens of any analysis project. But, now that we have a good suite of utilities for handling our Tweets, the remainder of our work goes quickly."]},{"cell_type":"markdown","metadata":{"id":"4p63VPh6i0Z8"},"source":["The code below will do the following for each Tweet in the dataset:\n","\n"," * Tokenize the text using the Tweet Tokenizer\n"," * Remove hyperlinks\n"," * Remove stopwords (standard English stopwords)\n"," * Remove punctuation tokens and strip @ and # from hashtags and mentions (see note below)\n"," * Lemmatize the remaining word tokens (using default noun part-of-speech for simplicity)\n","\n",".. and will collect the unique words and their counts into `word_counts`."]},{"cell_type":"markdown","metadata":{"id":"S2LfXhKnmcGM"},"source":["\u003e üí° Since this is a semantic network we are building, it seems useful to, e.g., treat **@Nike** and **Nike** as the same word. Hence, `strip_mentions`, and `strip_hashtags`. In some cases, for example a mentions network, you would probably take a different approach. As you preprocess and prepare data for the task at hand, it is important to be intentional and aware of how you are handling the text with your end goals in mind."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":126036,"status":"ok","timestamp":1678211638170,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"mSrz-9ofjsKs","outputId":"6c5a78d8-3b3d-4259-c0ae-061df91d3c9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processed 0 tweets\n","Processed 10000 tweets\n","Processed 20000 tweets\n","Processed 30000 tweets\n","Processed 40000 tweets\n","Processed 50000 tweets\n","Processed 60000 tweets\n","Processed 70000 tweets\n","Processed 80000 tweets\n","Processed 90000 tweets\n","Processed 100000 tweets\n","Processed 110000 tweets\n","Processed 120000 tweets\n","Processed 130000 tweets\n","Processed 140000 tweets\n","Processed 150000 tweets\n","Processed 160000 tweets\n","Processed 170000 tweets\n"]}],"source":["word_counts = {}\n","\n","with gzip.open(DATA_FILE) as data_file:\n","    for i, line in enumerate(data_file):\n","        if i % 10000 == 0:\n","            print(f\"Processed {i} tweets\")\n","        tweet = json.loads(line)\n","        text = tweet[\"full_text\"]\n","        tokens = tokenize(text, tweet=True)\n","        tokens = remove_links(tokens)\n","        tokens = remove_stopwords(tokens)\n","        tokens = remove_punctuation(tokens, strip_mentions=True, strip_hashtags=True)\n","        tokens = lemmatize(tokens) \n","        for word in tokens:\n","            if word not in word_counts:\n","                word_counts[word] = 0\n","            word_counts[word] += 1"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1678211638170,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"eIOWn78gn182","outputId":"5d0e87d9-2fcb-4fc6-f949-84c2d9848dde"},"outputs":[{"data":{"text/plain":["87055"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["len(word_counts)"]},{"cell_type":"markdown","metadata":{"id":"_800vRAzkdz0"},"source":["## Reducing the graph to the most common words"]},{"cell_type":"markdown","metadata":{"id":"eZFfT9jmkqyc"},"source":["To keep the size of your semantic network managable, reduce the word set to just the top 1000 most popular words.\n","\n","To do this, you will sort the word counts by reverse value (i.e. by count from highest to lowest) and take a slice of 1000."]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1678211638170,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"NXBjLQVsok7-"},"outputs":[],"source":["sorted_counts = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n","sorted_words = [word for word, count in sorted_counts]"]},{"cell_type":"markdown","metadata":{"id":"JMh06WnHpI1I"},"source":["Let's take a look at just a few of the top words:"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1678211638171,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"n0Jg27jIlSxU","outputId":"f88d7a6c-b797-4626-b52f-375fc62d90f5"},"outputs":[{"data":{"text/plain":["['nike',\n"," 'rt',\n"," '‚Ä¶',\n"," '‚Äô',\n"," 'adidas',\n"," 'xbox',\n"," 'sneakerscouts',\n"," 'eneskanter',\n"," 'day',\n"," 'via']"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["sorted_words[:10]"]},{"cell_type":"markdown","metadata":{"id":"2L4ZajBYpnoR"},"source":["Some things to note:\n","\n"," * There appears to be some punctuation here that made it through. We will leave it as both a thought exercise to consider why these tokens are here, and how you might clean them up.\n","\n"," * rt is right up there near the top, which is not surprising given that these are Tweets. This is an example of something you might clean up, for example, with a specialize stopword list. This cleanup is included below as a coding exercise.\n","\n"," * While Nike and Adidas made it to the top 10, Lululemon is not here. Why might that be? (The code snippet below sheds some light) And how would you deal with this if you wanted to include Lululemon in your analysis? (Hint: think about the segmentation work you did in the Topic Modeling course."]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1678211638171,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"Bre18RwAoVIZ","outputId":"f1210774-8314-4862-c219-9ba8e541d294"},"outputs":[{"name":"stdout","output_type":"stream","text":["Nike: 143755\n","Adidas: 39206\n","Lululemon: 6557\n"]}],"source":["print(\"Nike:\", word_counts[\"nike\"])\n","print(\"Adidas:\", word_counts[\"adidas\"])\n","print(\"Lululemon:\", word_counts[\"lululemon\"])"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":80752,"status":"ok","timestamp":1678211718919,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"nrBX_11gsbgO","outputId":"fa8d6225-b057-4ab9-8ae0-9c0392e732b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processed 0 tweets\n","Processed 10000 tweets\n","Processed 20000 tweets\n","Processed 30000 tweets\n","Processed 40000 tweets\n","Processed 50000 tweets\n","Processed 60000 tweets\n","Processed 70000 tweets\n","Processed 80000 tweets\n","Processed 90000 tweets\n","Processed 100000 tweets\n","Processed 110000 tweets\n","Processed 120000 tweets\n","Processed 130000 tweets\n","Processed 140000 tweets\n","Processed 150000 tweets\n","Processed 160000 tweets\n","Processed 170000 tweets\n"]}],"source":["word_counts = {}\n","\n","stopwords = [\"rt\"] + nltk.corpus.stopwords.words(\"english\") \n","\n","\n","with gzip.open(DATA_FILE) as data_file:\n","    for i, line in enumerate(data_file):\n","        if i % 10000 == 0:\n","            print(f\"Processed {i} tweets\")\n","        tweet = json.loads(line)\n","        text = tweet[\"full_text\"]\n","        tokens = tokenize(text, tweet=True)\n","        tokens = remove_links(tokens)\n","        tokens = remove_stopwords(tokens, stopwords=stopwords)\n","        tokens = remove_punctuation(tokens, strip_mentions=True, strip_hashtags=True)\n","        tokens = lemmatize(tokens) \n","        for word in tokens:\n","            if word not in word_counts:\n","                word_counts[word] = 0\n","            word_counts[word] += 1"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":37,"status":"ok","timestamp":1678211718920,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"C8Ie8EE9uIR8"},"outputs":[],"source":["sorted_counts = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n","sorted_words = [word for word, count in sorted_counts]"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1678211718920,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"mMNcQ3Taukkv","outputId":"ed27b98f-fce0-469b-e60e-3681096f707a"},"outputs":[{"data":{"text/plain":["['nike',\n"," '‚Ä¶',\n"," '‚Äô',\n"," 'adidas',\n"," 'xbox',\n"," 'sneakerscouts',\n"," 'eneskanter',\n"," 'day',\n"," 'via',\n"," 'available']"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["sorted_words[:10]"]},{"cell_type":"markdown","metadata":{"id":"fXE_X_l4wM-7"},"source":["## Build and plot the graph"]},{"cell_type":"markdown","metadata":{"id":"BIGiHGSqwPQc"},"source":["You have now done all the heavy lifting required to build the semantic network.\n","\n","The code below builds an undirected semantic network of co-occurring words that belong to our network of top n terms. These graphs can get kind of heavy, so start with a small graph of n=20 to keep things manageable."]},{"cell_type":"markdown","metadata":{"id":"m0o3dNnnx_s4"},"source":["To do this, we need to:\n","\n"," * Process each tweet in the same way we did previously\n"," * Determine which tokens in the Tweet belong to the top N\n"," * Add all of the 2-combinations (ie. co-occurrences) of included terms as an edge in the graph.\n","\n","We use the handy [itertools module](https://docs.python.org/3/library/itertools.html) to help us get this last thing done."]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84070,"status":"ok","timestamp":1678211802960,"user":{"displayName":"Joe Hardin","userId":"05754644729500922090"},"user_tz":300},"id":"PZGTuFSsw0NF","outputId":"03762780-3d6c-4aa3-974a-86f054df67e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processed 0 tweets\n","Just a glimpse so you can see what the cooccurrences for a tweet look like:\n","[('ad', 'nike'), ('ad', 'air'), ('ad', 'available'), ('ad', 'via'), ('ad', 'sneakerscouts'), ('ad', 'nike'), ('nike', 'air'), ('nike', 'available'), ('nike', 'via'), ('nike', 'sneakerscouts'), ('nike', 'nike'), ('air', 'available'), ('air', 'via'), ('air', 'sneakerscouts'), ('air', 'nike'), ('available', 'via'), ('available', 'sneakerscouts'), ('available', 'nike'), ('via', 'sneakerscouts'), ('via', 'nike'), ('sneakerscouts', 'nike')]\n","Processed 10000 tweets\n","Processed 20000 tweets\n","Processed 30000 tweets\n","Processed 40000 tweets\n","Processed 50000 tweets\n","Processed 60000 tweets\n","Processed 70000 tweets\n","Processed 80000 tweets\n","Processed 90000 tweets\n","Processed 100000 tweets\n","Processed 110000 tweets\n","Processed 120000 tweets\n","Processed 130000 tweets\n","Processed 140000 tweets\n","Processed 150000 tweets\n","Processed 160000 tweets\n","Processed 170000 tweets\n"]}],"source":["N = 20\n","top_terms = sorted_words[:N]\n","graph = nx.Graph()\n","\n","with gzip.open(DATA_FILE) as data_file:\n","    for i, line in enumerate(data_file):\n","        if i % 10000 == 0:\n","            print(f\"Processed {i} tweets\")\n","        tweet = json.loads(line)\n","        text = tweet[\"full_text\"]\n","        tokens = tokenize(text, tweet=True)\n","        tokens = remove_links(tokens)\n","        tokens = remove_stopwords(tokens, stopwords=stopwords)\n","        tokens = remove_punctuation(tokens, strip_mentions=True, strip_hashtags=True)\n","        tokens = lemmatize(tokens) \n","        \n","        # reduce the tweet to terms in the 1000 word network and add the\n","        # term relationships to the graph\n","        nodes = [t for t in tokens if t in top_terms]\n","        cooccurrences = itertools.combinations(nodes, 2)\n","        if i == 0:\n","            print(\"Just a glimpse so you can see what the cooccurrences for a tweet look like:\")\n","            cooccurrences = list(cooccurrences)\n","            print(cooccurrences)\n","        graph.add_edges_from(cooccurrences)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1OJNeEPNLtOJ_KBwGqybjzq9Sfq50Lurz"},"id":"TpwhcmSL7Bsz","outputId":"ff0ec557-a196-4275-b589-ea22853eb76d"},"outputs":[],"source":["fig, ax = plt.subplots(1, 1, figsize=(300, 300))\n","nx.draw_networkx(graph, ax=ax, font_color=\"#FFFFFF\", font_size=20, node_size=30000, width=4, arrowsize=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uPGwaf6cHnTe"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","provenance":[{"file_id":"1jK5H159F_Zonbqno_4249srkLSo1MPNt","timestamp":1678139897523}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}